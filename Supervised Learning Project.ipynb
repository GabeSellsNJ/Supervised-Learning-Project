{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRODUCTION\n",
    "We will be using machine learning models to predict whether a customer will leave Beta Bank soon. The data we have is on clientsâ€™ past behavior and termination of contracts with the bank.\n",
    "We will use various models and optimize parameters to obtain the most accruate machine learning model. Our goal is to acheive a F1 score of .59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#import requred python packages and import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('Churn.csv')\n",
    "\n",
    "#display dataframe info for preprocessing\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in missing values in the 'Tenure' column with the average (rouded) and change column to int type\n",
    "mean = round(df['Tenure'].mean())\n",
    "df['Tenure'] = df['Tenure'].fillna(mean)\n",
    "df['Tenure'] = df['Tenure'].astype(int)\n",
    "\n",
    "#for our machine learning models, we don't need these columns\n",
    "df = df.drop(['RowNumber','CustomerId','Surname'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CreditScore      10000 non-null  int64  \n",
      " 1   Age              10000 non-null  int64  \n",
      " 2   Tenure           10000 non-null  int64  \n",
      " 3   Balance          10000 non-null  float64\n",
      " 4   NumOfProducts    10000 non-null  int64  \n",
      " 5   HasCrCard        10000 non-null  int64  \n",
      " 6   IsActiveMember   10000 non-null  int64  \n",
      " 7   EstimatedSalary  10000 non-null  float64\n",
      " 8   Exited           10000 non-null  int64  \n",
      " 9   Geography        10000 non-null  float64\n",
      " 10  Gender           10000 non-null  float64\n",
      "dtypes: float64(4), int64(7)\n",
      "memory usage: 859.5 KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select the columns to encode\n",
    "cols_to_encode = ['Geography', 'Gender']\n",
    "\n",
    "# Create a new dataframe with only those columns\n",
    "data_to_encode = df[cols_to_encode]\n",
    "\n",
    "# Apply OrdinalEncoder to the new dataframe\n",
    "encoder = OrdinalEncoder()\n",
    "encoded_data = encoder.fit_transform(data_to_encode)\n",
    "\n",
    "# Convert the encoded data to a dataframe with the original column names\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=cols_to_encode)\n",
    "\n",
    "# Join the encoded columns back to the original dataframe\n",
    "df_encoded = df.drop(cols_to_encode, axis=1).join(encoded_df)\n",
    "\n",
    "# Now df_encoded contains all the original columns plus the encoded columns\n",
    "\n",
    "#checking our pre-processed and optimized for machine learning dataframe \n",
    "df_encoded.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe and series for the features and target\n",
    "features = df_encoded.drop(['Exited'], axis=1)\n",
    "target = df_encoded['Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 10)\n",
      "(2000, 10)\n",
      "(2000, 10)\n",
      "(6000,)\n",
      "(2000,)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "#make the 20% of the dataset into a testing dating set, and 80% for training\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=12345) \n",
    "\n",
    "#split 25% of traning set towards validation. Now we have 60/20/20 split for training, testing, and validation respectively\n",
    "features_train, features_val, target_train, target_val = train_test_split(\n",
    "    features_train, target_train, test_size=0.25, random_state=12345)\n",
    "\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "print(features_val.shape)\n",
    "\n",
    "print(target_train.shape)\n",
    "print(target_test.shape)\n",
    "print(target_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the continuous numerical features\n",
    "numeric = ['CreditScore', 'Age', 'Tenure',\n",
    "       'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_test[numeric] = scaler.transform(features_test[numeric])\n",
    "features_val[numeric] = scaler.transform(features_val[numeric])\n",
    "\n",
    "#this improved the logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1:\n",
    "\n",
    "First, we will use three different models (Logistic Regression, Decision Tree, and Random Forest) on our training dataset. WE WILL NOT FIX CLASS INBALANCE in this part.\n",
    "However after each model, we will alter the model's threshold and obtain the best F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.26 without altering model's thresold\n",
      "\n",
      "Best threshold: 0.24\n",
      "Best F1 score on validation set: 0.47\n",
      "F1 score on test set: 0.49\n",
      "AUC-ROC: 0.75\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression WITHOUT any balancing\n",
    "\n",
    "model_lg = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model_lg.fit(features_train, target_train)\n",
    "predicted_test = model_lg.predict(features_test)\n",
    "print('F1:', f1_score(target_test, predicted_test).round(2), \"without altering model's thresold\")\n",
    "print()\n",
    "\n",
    "# Predict on validation set and tune threshold\n",
    "best_f1_lg = 0\n",
    "best_threshold_lg = 0\n",
    "\n",
    "probs_valid_lg = model_lg.predict_proba(features_val)\n",
    "probs_one_valid_lg = probs_valid_lg[:, 1]\n",
    "\n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_lg = probs_one_valid_lg > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_lg)\n",
    "    if f1 > best_f1_lg:\n",
    "        best_f1_lg = f1\n",
    "        best_threshold_lg = threshold\n",
    "\n",
    "# Predict on test set using best threshold\n",
    "probs_test_lg = model_lg.predict_proba(features_test)\n",
    "probs_one_test_lg = probs_test_lg[:, 1]\n",
    "predicted_test_lg = probs_one_test_lg > best_threshold_lg\n",
    "best_f1_lg_test = f1_score(target_test, predicted_test_lg)\n",
    "\n",
    "print('Best threshold:', best_threshold_lg.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_lg.round(2))\n",
    "print('F1 score on test set:', best_f1_lg_test.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_lg).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5 with depth of 6\n",
      "\n",
      "Best threshold: 0.28\n",
      "Best F1 score on validation set: 0.57\n",
      "F1 score on test set: 0.61\n",
      "AUC-ROC: 0.84\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree WITHOUT any balancing\n",
    "\n",
    "best_depth = 0\n",
    "best_dt_score = 0\n",
    "for depth in range(1, 11):\n",
    "    dt_model = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    dt_model.fit(features_train, target_train)\n",
    "    dt_score_val = dt_model.score(features_val, target_val) #testing the model with the testing set\n",
    "    if dt_score_val > best_dt_score:\n",
    "        best_dt_score = dt_score_val#if new score is better or more accurate than previous score, we keep the new one. Otherwise, we we stick with the previous\n",
    "        best_depth = depth #same idea with depth\n",
    "        \n",
    "best_dt_model = DecisionTreeClassifier(random_state=12345,max_depth=best_depth)\n",
    "best_dt_model.fit(features_train, target_train)        \n",
    "\n",
    "predicted_test_dt = best_dt_model.predict(features_test)\n",
    "\n",
    "print('F1:',f1_score(target_test, predicted_test_dt).round(2), \"with depth of\", best_depth)\n",
    "print()\n",
    "\n",
    "# Predict on validation set and tune threshold\n",
    "best_f1_dt = 0\n",
    "best_threshold_dt = 0\n",
    "\n",
    "probs_valid_dt = best_dt_model.predict_proba(features_val)\n",
    "probs_one_valid_dt = probs_valid_dt[:, 1]\n",
    "\n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_dt = probs_one_valid_dt > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_dt)\n",
    "    if f1 > best_f1_dt:\n",
    "        best_f1_dt = f1\n",
    "        best_threshold_dt = threshold        \n",
    "               \n",
    "# Predict on test set using best threshold\n",
    "probs_test_dt = best_dt_model.predict_proba(features_test)\n",
    "probs_one_test_dt = probs_test_dt[:, 1]\n",
    "predicted_test_dt = probs_one_test_dt > best_threshold_dt\n",
    "best_f1_dt_test = f1_score(target_test, predicted_test_dt)\n",
    "\n",
    "print('Best threshold:', best_threshold_dt.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_dt.round(2))\n",
    "print('F1 score on test set:', best_f1_dt_test.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_dt).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.53 with est of 14\n",
      "\n",
      "Best threshold: 0.44\n",
      "Best F1 score on validation set: 0.55\n",
      "F1 score on test set: 0.56\n",
      "AUC-ROC: 0.83\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier WITHOUT any balancing\n",
    "\n",
    "best_score = 0\n",
    "best_est = 0\n",
    "for est in range(1, 21):\n",
    "    forest_model = RandomForestClassifier(random_state=12345, n_estimators=est)\n",
    "    forest_model.fit(features_train, target_train)\n",
    "    forest_score_val = forest_model.score(features_val, target_val)\n",
    "    if forest_score_val > best_score:\n",
    "        best_score = forest_score_val #if new score is better or more accurate than previous score, we keep the new one. Otherwise, we we stick with the previous\n",
    "        best_est = est #same thing with est\n",
    "\n",
    "final_forest_model = RandomForestClassifier(random_state=12345, n_estimators=best_est)\n",
    "final_forest_model.fit(features_train, target_train)\n",
    "\n",
    "predicted_test = final_forest_model.predict(features_test)\n",
    "\n",
    "print('F1:',f1_score(target_test, predicted_test).round(2), \"with est of\", best_est)\n",
    "print()\n",
    "\n",
    "# Predict on validation set and tune threshold\n",
    "best_f1_rf = 0\n",
    "best_threshold_rf = 0\n",
    "\n",
    "probs_valid_rf = final_forest_model.predict_proba(features_val)\n",
    "probs_one_valid_rf = probs_valid_rf[:, 1]\n",
    "\n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_rf = probs_one_valid_rf > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_rf)\n",
    "    if f1 > best_f1_rf:\n",
    "        best_f1_rf = f1\n",
    "        best_threshold_rf = threshold         \n",
    "               \n",
    "# Predict on test set using best threshold\n",
    "probs_test_rf = final_forest_model.predict_proba(features_test)\n",
    "probs_one_test_rf = probs_test_rf[:, 1]\n",
    "predicted_test_rf = probs_one_test_rf > best_threshold_rf\n",
    "best_f1_rf_test = f1_score(target_test, predicted_test_rf)\n",
    "\n",
    "print('Best threshold:', best_threshold_rf.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_rf.round(2))\n",
    "print('F1 score on test set:', best_f1_rf_test.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_rf).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 Conclusion:\n",
    "\n",
    "Altering the thresholds of each model resulted in higher F1 scores, thus meaning, higher quality models.\n",
    "Decision Tree preformed better than Random Forest. Decision Tee also had a slightly higher AUC-ROC score.\n",
    "Decision Tree is the only model who acheived a F1 Score highter than .59.\n",
    "Logistic Regression preformed the worst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2:\n",
    "\n",
    "We will use the excact same models, but will use the hyperparameter class_weight ='balanced' in each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.48 without altering model's thresold\n",
      "\n",
      "Best threshold: 0.58\n",
      "Best F1 score on validation set: 0.48\n",
      "F1 score on test set: 0.4\n",
      "AUC-ROC: 0.75\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression wiith class_weight='balanced'\n",
    "\n",
    "model_lg_bal = LogisticRegression(random_state=12345, class_weight ='balanced', solver='liblinear')\n",
    "model_lg_bal.fit(features_train, target_train)\n",
    "predicted_test_bal = model_lg_bal.predict(features_test)\n",
    "print('F1:', f1_score(target_test, predicted_test_bal).round(2), \"without altering model's thresold\")\n",
    "print()\n",
    "\n",
    "# Predict on validation set and tune threshold\n",
    "best_f1_lg_bal = 0\n",
    "best_threshold_lg_bal = 0\n",
    "\n",
    "probs_valid_lg_bal = model_lg_bal.predict_proba(features_val)\n",
    "probs_one_valid_lg_bal = probs_valid_lg_bal[:, 1]\n",
    "\n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_lg_bal = probs_one_valid_lg_bal > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_lg_bal)\n",
    "    if f1 > best_f1_lg_bal:\n",
    "        best_f1_lg_bal = f1\n",
    "        best_threshold_lg_bal = threshold\n",
    "\n",
    "# Predict on test set using best threshold\n",
    "probs_test_lg_bal = model_lg_bal.predict_proba(features_test)\n",
    "probs_one_test_lg_bal = probs_test_lg_bal[:, 1]\n",
    "predicted_test_lg_bal = probs_one_test_lg_bal > best_threshold_lg\n",
    "best_f1_lg_test_bal = f1_score(target_test, predicted_test_lg_bal)\n",
    "\n",
    "print('Best threshold:', best_threshold_lg_bal.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_lg_bal.round(2))\n",
    "print('F1 score on test set:', best_f1_lg_test_bal.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_lg_bal).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5 with depth of 6\n",
      "\n",
      "Best threshold: 0.28\n",
      "Best F1 score on validation set: 0.57\n",
      "F1 score on test set: 0.61\n",
      "AUC-ROC: 0.84\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree with class_weight='balanced'\n",
    "\n",
    "best_depth = 0\n",
    "best_dt_score_bal = 0\n",
    "for depth in range(1, 11):\n",
    "    dt_model_bal = DecisionTreeClassifier(random_state=12345, class_weight ='balanced', max_depth=depth)\n",
    "    dt_model_bal.fit(features_train, target_train)\n",
    "    dt_score_val_bal = dt_model_bal.score(features_val, target_val) #testing the model with the testing set\n",
    "    if dt_score_val_bal > best_dt_score_bal:\n",
    "        best_dt_score_bal = dt_score_val_bal#if new score is better or more accurate than previous score, we keep the new one. Otherwise, we we stick with the previous\n",
    "        best_depth = depth #same idea with depth\n",
    "        \n",
    "best_dt_model_bal = DecisionTreeClassifier(random_state=12345,max_depth=best_depth)\n",
    "best_dt_model_bal.fit(features_train, target_train)        \n",
    "\n",
    "predicted_test_dt_bal = best_dt_model_bal.predict(features_test)\n",
    "\n",
    "print('F1:',f1_score(target_test, predicted_test_dt_bal).round(2), \"with depth of\", best_depth)\n",
    "print()\n",
    "\n",
    "# Predict on validation set and tune threshold\n",
    "best_f1_dt_bal = 0\n",
    "best_threshold_dt_bal = 0\n",
    "\n",
    "probs_valid_dt_bal = best_dt_model_bal.predict_proba(features_val)\n",
    "probs_one_valid_dt_bal = probs_valid_dt[:, 1]\n",
    "\n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_dt_bal = probs_one_valid_dt_bal > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_dt_bal)\n",
    "    if f1 > best_f1_dt_bal:\n",
    "        best_f1_dt_bal = f1\n",
    "        best_threshold_dt_bal = threshold        \n",
    "               \n",
    "# Predict on test set using best threshold\n",
    "probs_test_dt_bal = best_dt_model_bal.predict_proba(features_test)\n",
    "probs_one_test_dt_bal = probs_test_dt_bal[:, 1]\n",
    "predicted_test_dt_bal = probs_one_test_dt_bal > best_threshold_dt_bal\n",
    "best_f1_dt_test_bal = f1_score(target_test, predicted_test_dt_bal)\n",
    "\n",
    "print('Best threshold:', best_threshold_dt_bal.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_dt_bal.round(2))\n",
    "print('F1 score on test set:', best_f1_dt_test_bal.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_dt_bal).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.48 with est of 16\n",
      "\n",
      "Best threshold: 0.38\n",
      "Best F1 score on validation set: 0.56\n",
      "F1 score on test set: 0.59\n",
      "AUC-ROC: 0.83\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier with class_weight='balanced'\n",
    "\n",
    "best_score = 0\n",
    "best_est = 0\n",
    "for est in range(1, 21):\n",
    "    forest_model_bal = RandomForestClassifier(random_state=12345, class_weight='balanced', n_estimators=est)\n",
    "    forest_model_bal.fit(features_train, target_train)\n",
    "    forest_score_val_bal = forest_model_bal.score(features_val, target_val)\n",
    "    if forest_score_val_bal > best_score:\n",
    "        best_score = forest_score_val_bal #if new score is better or more accurate than previous score, we keep the new one. Otherwise, we we stick with the previous\n",
    "        best_est = est #same thing with est\n",
    "\n",
    "final_forest_model_bal = RandomForestClassifier(random_state=12345, n_estimators=best_est)\n",
    "final_forest_model_bal.fit(features_train, target_train)\n",
    "\n",
    "predicted_test = final_forest_model_bal.predict(features_test)\n",
    "\n",
    "print('F1:',f1_score(target_test, predicted_test_bal).round(2), \"with est of\", best_est)\n",
    "print()\n",
    "\n",
    "# Predict on validation set and tune threshold\n",
    "best_f1_rf_bal = 0\n",
    "best_threshold_rf_bal = 0\n",
    "\n",
    "probs_valid_rf_bal = final_forest_model_bal.predict_proba(features_val)\n",
    "probs_one_valid_rf_bal = probs_valid_rf_bal[:, 1]\n",
    "\n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_rf_bal = probs_one_valid_rf_bal > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_rf_bal)\n",
    "    if f1 > best_f1_rf_bal:\n",
    "        best_f1_rf_bal = f1\n",
    "        best_threshold_rf_bal = threshold         \n",
    "               \n",
    "# Predict on test set using best threshold\n",
    "probs_test_rf_bal = final_forest_model_bal.predict_proba(features_test)\n",
    "probs_one_test_rf_bal = probs_test_rf_bal[:, 1]\n",
    "predicted_test_rf_bal = probs_one_test_rf_bal > best_threshold_rf_bal\n",
    "best_f1_rf_test_bal = f1_score(target_test, predicted_test_rf_bal)\n",
    "\n",
    "print('Best threshold:', best_threshold_rf_bal.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_rf_bal.round(2))\n",
    "print('F1 score on test set:', best_f1_rf_test_bal.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_rf_bal).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 Conclusion:\n",
    "\n",
    "Logistic Regression Model yielded much worse results. Decision Tree remained the same. Random Forest improved slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4781, 10)\n",
      "(1219, 10)\n",
      "(4781,)\n",
      "(1219,)\n"
     ]
    }
   ],
   "source": [
    "#checking the inbalance of classes\n",
    "features_zeros = features_train[target_train == 0]\n",
    "features_ones = features_train[target_train == 1]\n",
    "target_zeros = target_train[target_train == 0]\n",
    "target_ones = target_train[target_train == 1]\n",
    "\n",
    "print(features_zeros.shape)\n",
    "print(features_ones.shape)\n",
    "print(target_zeros.shape)\n",
    "print(target_ones.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the there are almost four times more negative classes than positive.\n",
    "For our upsampling section, we will increase the 'ones' or positive class by a multiplication factor of 4. \n",
    "For our downsampling section, we will decreae the 'zero' or negative class by a factor of .25. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3:\n",
    "\n",
    "We will use the same three model, but we will fix inbalancing by upsampling.\n",
    "Once again after each model, we will alter the model's threshold and obtain the best F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating function for upsample\n",
    "\n",
    "def upsample(features, target, repeat):\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=12345)\n",
    "\n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "#upsampled training sets\n",
    "features_upsampled, target_upsampled = upsample(features_train, target_train, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.48\n",
      "\n",
      "Best threshold: 0.54\n",
      "Best F1 score on validation set: 0.48\n",
      "F1 score on test set: 0.49\n",
      "AUC-ROC: 0.75\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression with Upsampling\n",
    "\n",
    "model_lg_up = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model_lg_up.fit(features_upsampled, target_upsampled)\n",
    "predicted_test_lg_up = model_lg_up.predict(features_test)\n",
    "print('F1:', f1_score(target_test, predicted_test_lg_up).round(2))\n",
    "print()\n",
    "\n",
    "# Predict on validation set and tune threshold\n",
    "best_f1_lg_up = 0\n",
    "best_threshold_lg_up = 0\n",
    "\n",
    "probs_valid_lg_up = model_lg_up.predict_proba(features_val)\n",
    "probs_one_valid_lg_up = probs_valid_lg_up[:, 1]\n",
    "\n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_lg_up = probs_one_valid_lg_up > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_lg_up)\n",
    "    if f1 > best_f1_lg_up:\n",
    "        best_f1_lg_up = f1\n",
    "        best_threshold_lg_up = threshold\n",
    "\n",
    "# Predict on test set using best threshold\n",
    "probs_test_lg_up = model_lg_up.predict_proba(features_test)\n",
    "probs_one_test_lg_up = probs_test_lg_up[:, 1]\n",
    "predicted_test_lg_up = probs_one_test_lg_up > best_threshold_lg_up\n",
    "best_f1_lg_test_up = f1_score(target_test, predicted_test_lg_up)\n",
    "\n",
    "print('Best threshold:', best_threshold_lg_up.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_lg_up.round(2))\n",
    "print('F1 score on test set:', best_f1_lg_test_up.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_lg_up).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.59 with depth of 6\n",
      "\n",
      "Best threshold: 0.56\n",
      "Best F1 score on validation set: 0.57\n",
      "F1 score on test set: 0.48\n",
      "AUC-ROC: 0.83\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree with Upsampling \n",
    "\n",
    "best_depth = 0\n",
    "best_dt_up_score = 0\n",
    "for depth in range(1, 11):\n",
    "    dt_model_up = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    dt_model_up.fit(features_upsampled, target_upsampled)\n",
    "    dt_up_score_val = dt_model_up.score(features_val, target_val) #testing the model_up with the testing set\n",
    "    if dt_up_score_val > best_dt_up_score:\n",
    "        best_dt_up_score = dt_up_score_val#if new score is better or more accurate than previous score, we keep the new one. Otherwise, we we stick with the previous\n",
    "        best_depth = depth #same idea with depth\n",
    "        \n",
    "best_dt_model_up = DecisionTreeClassifier(random_state=12345,max_depth=best_depth)\n",
    "best_dt_model_up.fit(features_upsampled, target_upsampled)        \n",
    "\n",
    "predicted_test_dt_up = best_dt_model_up.predict(features_test)\n",
    "\n",
    "print('F1:',f1_score(target_test, predicted_test_dt_up).round(2), \"with depth of\", best_depth)\n",
    "print()\n",
    "\n",
    "# Predict on validation set and tune threshold\n",
    "best_f1_dt_up = 0\n",
    "best_threshold_dt_up = 0\n",
    "\n",
    "probs_valid_dt_up = best_dt_model_up.predict_proba(features_val)\n",
    "probs_one_valid_dt_up = probs_valid_dt_up[:, 1]\n",
    "    \n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_dt_up = probs_one_valid_dt_up > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_dt_up)\n",
    "    if f1 > best_f1_dt_up:\n",
    "        best_f1_dt_up = f1\n",
    "        best_threshold_dt_up = threshold    \n",
    "    \n",
    "# Predict on test set using best threshold\n",
    "probs_test_dt_up = best_dt_model_up.predict_proba(features_test)\n",
    "probs_one_test_dt_up = probs_test_dt_up[:, 1]\n",
    "predicted_test_dt_up = probs_one_test_dt_up > best_threshold_dt\n",
    "best_f1_dt_test_up = f1_score(target_test, predicted_test_dt_up)\n",
    "\n",
    "print('Best threshold:', best_threshold_dt_up.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_dt_up.round(2))\n",
    "print('F1 score on test set:', best_f1_dt_test_up.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_dt_up).round(2))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.54 with est of 18\n",
      "\n",
      "Best threshold: 0.34\n",
      "Best F1 score on validation set: 0.57\n",
      "F1 score on test set: 0.56\n",
      "AUC-ROC: 0.83\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier with Upsampling \n",
    "\n",
    "best_score = 0\n",
    "best_est = 0\n",
    "for est in range(1, 31):\n",
    "    forest_model_up = RandomForestClassifier(random_state=12345, n_estimators=est)\n",
    "    forest_model_up.fit(features_upsampled, target_upsampled)\n",
    "    forest_score_val = forest_model_up.score(features_val, target_val)\n",
    "    if forest_score_val > best_score:\n",
    "        best_score = forest_score_val #if new score is better or more accurate than previous score, we keep the new one. Otherwise, we we stick with the previous\n",
    "        best_est = est #same thing with est\n",
    "\n",
    "final_forest_model_up = RandomForestClassifier(random_state=12345, n_estimators=best_est)\n",
    "final_forest_model_up.fit(features_train, target_train)\n",
    "\n",
    "predicted_test_rf_up = final_forest_model_up.predict(features_test)\n",
    "\n",
    "print('F1:',f1_score(target_test, predicted_test_rf_up).round(2), \"with est of\", best_est)\n",
    "print()\n",
    "\n",
    "# Predict on validation set and tune threshold\n",
    "best_f1_rf_up = 0\n",
    "best_threshold_rf_up = 0\n",
    "\n",
    "probs_valid_rf_up = final_forest_model_up.predict_proba(features_val)\n",
    "probs_one_valid_rf_up = probs_valid_rf_up[:, 1]\n",
    "      \n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_rf_up = probs_one_valid_rf_up > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_rf_up)\n",
    "    if f1 > best_f1_rf_up:\n",
    "        best_f1_rf_up = f1\n",
    "        best_threshold_rf_up = threshold         \n",
    "               \n",
    "# Predict on test set using best threshold\n",
    "probs_test_rf_up = final_forest_model.predict_proba(features_test)\n",
    "probs_one_test_rf_up = probs_test_rf_up[:, 1]\n",
    "predicted_test_rf_up = probs_one_test_rf_up > best_threshold_rf_up\n",
    "best_f1_rf_test_up = f1_score(target_test, predicted_test_rf)\n",
    "\n",
    "print('Best threshold:', best_threshold_rf_up.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_rf_up.round(2))\n",
    "print('F1 score on test set:', best_f1_rf_test_up.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_rf_up).round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3 Conclusion:\n",
    "\n",
    "This time Random Forest preformed the best, but still under our desired F1 score of .59 and worse than when weight_class = 'balanced'. \n",
    "Decision Tree's performance dratiscally decreased, even slightly worse than Logistic Regression. Logistic Regression preformed the same as when the dataset was unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4:\n",
    "\n",
    "We will use the same three model, but we will fix inbalancing by downsampling.\n",
    "Once again after each model, we will alter the model's threshold and obtain the best F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating function for downsample\n",
    "\n",
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat([features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    \n",
    "    target_downsampled = pd.concat([target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "\n",
    "    features_downsampled, target_downsampled = shuffle(features_downsampled, target_downsampled, random_state=12345)\n",
    "\n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "#down sampled training sets\n",
    "features_downsampled, target_downsampled = downsample(features_train, target_train, .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.49\n",
      "\n",
      "Best threshold: 0.54\n",
      "Best F1 score on validation set: 0.48\n",
      "F1 score on test set: 0.49\n",
      "AUC-ROC: 0.76\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression with Downsampling\n",
    "\n",
    "model_lg_down = LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model_lg_down.fit(features_downsampled, target_downsampled)\n",
    "predicted_test_lg_down = model_lg_down.predict(features_test)\n",
    "print('F1:', f1_score(target_test, predicted_test_lg_down).round(2))\n",
    "print()\n",
    "\n",
    "# Predict on validation set and tune threshold\n",
    "best_f1_lg_down = 0\n",
    "best_threshold_lg_down = 0\n",
    "\n",
    "probs_valid_lg_down = model_lg_down.predict_proba(features_val)\n",
    "probs_one_valid_lg_down = probs_valid_lg_down[:, 1]\n",
    "\n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_lg_down = probs_one_valid_lg_down > threshold\n",
    "    if f1_score(target_val, predicted_valid_lg_down) > best_f1_lg_down:\n",
    "        best_f1_lg_down = f1_score(target_val, predicted_valid_lg_down)\n",
    "        best_threshold_lg_down = threshold\n",
    "\n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_lg_down = probs_one_valid_lg_down > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_lg_down)\n",
    "    if f1 > best_f1_lg_down:\n",
    "        best_f1_lg_down = f1\n",
    "        best_threshold_lg_down = threshold\n",
    "\n",
    "# Predict on test set using best threshold\n",
    "probs_test_lg_down = model_lg_down.predict_proba(features_test)\n",
    "probs_one_test_lg_down = probs_test_lg_down[:, 1]\n",
    "predicted_test_lg_down = probs_one_test_lg_down > best_threshold_lg_down\n",
    "best_f1_lg_test_down = f1_score(target_test, predicted_test_lg_down)\n",
    "\n",
    "print('Best threshold:', best_threshold_lg_down.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_lg_down.round(2))\n",
    "print('F1 score on test set:', best_f1_lg_test_down.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_lg_down).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5416227608008429 with depth of 4\n",
      "\n",
      "Best threshold: 0.5\n",
      "Best F1 score on validation set: 0.51\n",
      "F1 score on test set: 0.46\n",
      "AUC-ROC: 0.81\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree with Downsampling \n",
    "\n",
    "best_depth = 0\n",
    "best_dt_down_score = 0\n",
    "for depth in range(1, 11):\n",
    "    dt_model_down = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    dt_model_down.fit(features_downsampled, target_downsampled)\n",
    "    dt_down_score_val = dt_model_down.score(features_val, target_val) #testing the model_down with the testing set\n",
    "    if dt_down_score_val > best_dt_down_score:\n",
    "        best_dt_down_score = dt_down_score_val#if new score is better or more accurate than previous score, we keep the new one. Otherwise, we we stick with the previous\n",
    "        best_depth = depth #same idea with depth\n",
    "        \n",
    "best_dt_model_down = DecisionTreeClassifier(random_state=12345,max_depth=best_depth)\n",
    "best_dt_model_down.fit(features_downsampled, target_downsampled)        \n",
    "\n",
    "predicted_test_dt_down = best_dt_model_down.predict(features_test)\n",
    "\n",
    "print('F1:',f1_score(target_test, predicted_test_dt_down), \"with depth of\", best_depth)\n",
    "print()\n",
    "\n",
    "# Predict on test set using best threshold\n",
    "best_f1_dt_down = 0\n",
    "best_threshold_dt_down = 0\n",
    "\n",
    "probs_valid_dt_down = best_dt_model_down.predict_proba(features_val)\n",
    "probs_one_valid_dt_down = probs_valid_dt_down[:, 1]\n",
    "    \n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_dt_down = probs_one_valid_dt_down > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_dt_down)\n",
    "    if f1 > best_f1_dt_down:\n",
    "        best_f1_dt_down = f1\n",
    "        best_threshold_dt_down = threshold    \n",
    "    \n",
    "probs_test_dt_down = best_dt_model_down.predict_proba(features_test)\n",
    "probs_one_test_dt_down = probs_test_dt_down[:, 1]\n",
    "predicted_test_dt_down = probs_one_test_dt_down > best_threshold_dt\n",
    "best_f1_dt_test_down = f1_score(target_test, predicted_test_dt_down)\n",
    "\n",
    "print('Best threshold:', best_threshold_dt_down.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_dt_down.round(2))\n",
    "print('F1 score on test set:', best_f1_dt_test_down.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_dt_down).round(2))\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.56 with est of 30\n",
      "\n",
      "Best threshold: 0.34\n",
      "Best F1 score on validation set: 0.58\n",
      "F1 score on test set: 0.56\n",
      "AUC-ROC: 0.83\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier with Downsampling \n",
    "\n",
    "best_score = 0\n",
    "best_est = 0\n",
    "for est in range(1, 31):\n",
    "    forest_model_rf_down = RandomForestClassifier(random_state=12345, n_estimators=est)\n",
    "    forest_model_rf_down.fit(features_downsampled, target_downsampled)\n",
    "    forest_score_val = forest_model_rf_down.score(features_val, target_val)\n",
    "    if forest_score_val > best_score:\n",
    "        best_score = forest_score_val #if new score is better or more accurate than previous score, we keep the new one. Otherwise, we we stick with the previous\n",
    "        best_est = est #same thing with est\n",
    "\n",
    "final_forest_model_rf_down = RandomForestClassifier(random_state=12345, n_estimators=best_est)\n",
    "final_forest_model_rf_down.fit(features_train, target_train)\n",
    "\n",
    "predicted_test_rf_down = final_forest_model_rf_down.predict(features_test)\n",
    "\n",
    "print('F1:',f1_score(target_test, predicted_test_rf_down).round(2), \"with est of\", best_est)\n",
    "print()\n",
    "\n",
    "# Predict on validation set and tune threshold\n",
    "best_f1_rf_down = 0\n",
    "best_threshold_rf_down = 0\n",
    "\n",
    "probs_valid_rf_down = final_forest_model_rf_down.predict_proba(features_val)\n",
    "probs_one_valid_rf_down = probs_valid_rf_down[:, 1]\n",
    "\n",
    "for threshold in np.arange(0, 0.8, 0.02):\n",
    "    predicted_valid_rf_down = probs_one_valid_rf_down > threshold\n",
    "    f1 = f1_score(target_val, predicted_valid_rf_down)\n",
    "    if f1 > best_f1_rf_down:\n",
    "        best_f1_rf_down = f1\n",
    "        best_threshold_rf_down = threshold         \n",
    "               \n",
    "# Predict on test set using best threshold\n",
    "probs_test_rf_down = final_forest_model.predict_proba(features_test)\n",
    "probs_one_test_rf_down = probs_test_rf_down[:, 1]\n",
    "predicted_test_rf_down = probs_one_test_rf_down > best_threshold_rf_down\n",
    "best_f1_rf_test_down = f1_score(target_test, predicted_test_rf)\n",
    "\n",
    "print('Best threshold:', best_threshold_rf_down.round(2))\n",
    "print('Best F1 score on validation set:', best_f1_rf_down.round(2))\n",
    "print('F1 score on test set:', best_f1_rf_test_down.round(2))\n",
    "print('AUC-ROC:', roc_auc_score(target_test, probs_one_test_rf_down).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4 Conclusion:\n",
    "\n",
    "Once again Random Forest preformed the best, but still under our desired F1 score of .59 and worse than when weight_class = 'balanced'. \n",
    "Decision Tree's performance decreased. Logistic Regression's performance did not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Logistic Regression  Decision Tree  Random Forest\n",
      "No Balancing                            0.49           0.61           0.56\n",
      "class_weight = balanced                 0.40           0.61           0.59\n",
      "Upsampled                               0.49           0.48           0.56\n",
      "Downsampled                             0.49           0.46           0.56\n"
     ]
    }
   ],
   "source": [
    "#presenting all the results in a clear 3x3 matrix\n",
    "data = {'Logistic Regression': [best_f1_lg_test, best_f1_lg_test_bal, best_f1_lg_test_up, best_f1_lg_test_down],\n",
    "        'Decision Tree': [best_f1_dt_test, best_f1_dt_test_bal, best_f1_dt_test_up, best_f1_dt_test_down],\n",
    "        'Random Forest': [best_f1_rf_test, best_f1_rf_test_bal, best_f1_rf_test_up, best_f1_rf_test_down]}\n",
    "analysis_matrix = pd.DataFrame(data, index=['No Balancing', 'class_weight = balanced', 'Upsampled', 'Downsampled'])\n",
    "analysis_matrix = analysis_matrix.round(2) \n",
    "print(analysis_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL CONCLUSION\n",
    "The winning model is Decision Tree since it's the only model which acheived our goal of a F1 score higher than .59.\n",
    "These results are interesting. Decision Tree was only accurate with the orignal unbalanced dataset or when class_weight was balanced. Upsampling or Downsampling the dataset, made the model the worst one!\n",
    "Random Forest is the runner up. Random Forest acheived consistent results despite balancing changes to the dataset. Maybe with more tuning or other changes to parameters, it could be prove to be more accurate than Decision Tree.\n",
    "\n",
    "All of the AUC-ROC values are consistent for each Model, respectively. We yielded median scores of .75 for Logistic Regression and as high as .84 for Decision Tree and Random Forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "128a52faa6f19e8d897e7a909837098490dae80bb9f013db4ed13fafd5e25653"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
